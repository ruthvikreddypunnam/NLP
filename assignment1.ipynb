{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b59d6",
   "metadata": {},
   "source": [
    "Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you \n",
    "looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? \n",
    "Machines, after all, recognize numbers, not the letters of our language. And that can \n",
    "be a tricky landscape to navigate in machine learning.\n",
    "Solving  an  NLP  problem  is  a  multi-stage  process.  We  need  to  clean  the  unstructured  text  data  first \n",
    "before we can even think about getting to the modeling stage. Cleaning the data consists of a few key \n",
    "steps:\n",
    "1. Split the above paragraph into sentences\n",
    "2. Split the above paragraph into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd4893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"Are you fascinated by the amount of text data available on the internet? Are you looking for ways to work with this text data but aren’t sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning. Solving an NLP problem is a multi-stage process. We need to clean the unstructured text data first before we can even think about getting to the modeling stage. Cleaning the data consists of a few key steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8091760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bcfe16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are you fascinated by the amount of text data available on the internet?', 'Are you looking for ways to work with this text data but aren’t sure where to begin?', 'Machines, after all, recognize numbers, not the letters of our language.', 'And that can be a tricky landscape to navigate in machine learning.', 'Solving an NLP problem is a multi-stage process.', 'We need to clean the unstructured text data first before we can even think about getting to the modeling stage.', 'Cleaning the data consists of a few key steps']\n"
     ]
    }
   ],
   "source": [
    "token_text=sent_tokenize(para)\n",
    "print(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3594ca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you fascinated by the amount of text data available on the internet?\n",
      "Are you looking for ways to work with this text data but aren’t sure where to begin?\n",
      "Machines, after all, recognize numbers, not the letters of our language.\n",
      "And that can be a tricky landscape to navigate in machine learning.\n",
      "Solving an NLP problem is a multi-stage process.\n",
      "We need to clean the unstructured text data first before we can even think about getting to the modeling stage.\n",
      "Cleaning the data consists of a few key steps\n"
     ]
    }
   ],
   "source": [
    "for i in token_text:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400994de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet', '?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren', '’', 't', 'sure', 'where', 'to', 'begin', '?', 'Machines', ',', 'after', 'all', ',', 'recognize', 'numbers', ',', 'not', 'the', 'letters', 'of', 'our', 'language', '.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning', '.', 'Solving', 'an', 'NLP', 'problem', 'is', 'a', 'multi-stage', 'process', '.', 'We', 'need', 'to', 'clean', 'the', 'unstructured', 'text', 'data', 'first', 'before', 'we', 'can', 'even', 'think', 'about', 'getting', 'to', 'the', 'modeling', 'stage', '.', 'Cleaning', 'the', 'data', 'consists', 'of', 'a', 'few', 'key', 'steps']\n"
     ]
    }
   ],
   "source": [
    "word_tokens=word_tokenize(para)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea536cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Are',\n",
       " 'you',\n",
       " 'fascinated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'text',\n",
       " 'data',\n",
       " 'available',\n",
       " 'on',\n",
       " 'the',\n",
       " 'internet?',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'ways',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'this',\n",
       " 'text',\n",
       " 'data',\n",
       " 'but',\n",
       " 'aren’t',\n",
       " 'sure',\n",
       " 'where',\n",
       " 'to',\n",
       " 'begin?',\n",
       " 'Machines,',\n",
       " 'after',\n",
       " 'all,',\n",
       " 'recognize',\n",
       " 'numbers,',\n",
       " 'not',\n",
       " 'the',\n",
       " 'letters',\n",
       " 'of',\n",
       " 'our',\n",
       " 'language.',\n",
       " 'And',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'tricky',\n",
       " 'landscape',\n",
       " 'to',\n",
       " 'navigate',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning.',\n",
       " 'Solving',\n",
       " 'an',\n",
       " 'NLP',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'a',\n",
       " 'multi-stage',\n",
       " 'process.',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'data',\n",
       " 'first',\n",
       " 'before',\n",
       " 'we',\n",
       " 'can',\n",
       " 'even',\n",
       " 'think',\n",
       " 'about',\n",
       " 'getting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'modeling',\n",
       " 'stage.',\n",
       " 'Cleaning',\n",
       " 'the',\n",
       " 'data',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'a',\n",
       " 'few',\n",
       " 'key',\n",
       " 'steps']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3720e",
   "metadata": {},
   "source": [
    "3. Find stem and lemma words for the given words?\n",
    "“cats\"\n",
    "\"trouble\"\n",
    "\"troubling\"\n",
    "\"troubled\"\n",
    "“having”\n",
    "“Corriendo”\n",
    "“at”\n",
    "“was”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dd6e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9d4d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "have\n",
      "corriendo\n",
      "at\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "for i in words:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88e835ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "trouble\n",
      "troubling\n",
      "troubled\n",
      "having\n",
      "Corriendo\n",
      "at\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "ps=WordNetLemmatizer()\n",
    "for i in words:\n",
    "    print(ps.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b194df",
   "metadata": {},
   "source": [
    "4. Find stop words from the given paragraph?\n",
    "“The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for \n",
    "Natural Language Processing. NLTK supports stop word removal, and you can find the list \n",
    "of stop words in the  corpus  module. To remove stop words from a sentence, you can divide \n",
    "your text into words and then remove the word if it exits in the list of stop words provided \n",
    "by NLTK.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77c2d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "635b15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d1a7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can', \"don't\", 'hasn', 'shan', 'while', 're', 'myself', \"you'll\", 'no', 'should', 'she', 'doing', 'few', 'having', 'just', 'so', 'does', \"shan't\", \"mustn't\", 'of', \"needn't\", 'her', 'if', 'had', 'but', 'other', 'up', 'theirs', 'ma', 'what', 'their', 'yourselves', \"should've\", \"aren't\", \"she's\", 'its', 'o', 'against', 'most', 'ourselves', 'too', 'mightn', 'his', 'again', 'he', 'once', 'ours', 'before', 'some', 'or', 'why', 'me', 'these', 'am', 'not', 'during', 'out', \"you've\", 'off', 'after', \"couldn't\", 'how', 'there', \"hasn't\", 'at', 'hadn', 'wasn', 'between', 'whom', \"you'd\", 'the', 'because', 'is', 'on', \"won't\", \"haven't\", 'y', \"that'll\", 'don', 'they', 'are', 'didn', 'which', 'to', 'needn', 'yourself', 'for', 'very', 's', \"wasn't\", 'about', 'your', 'be', 'both', 'nor', 'doesn', 'own', 'isn', 've', 'below', \"shouldn't\", 'above', 'all', 'until', \"doesn't\", 'more', 'over', 'being', 'as', 'here', 'and', 'himself', 'each', 'same', 'where', 'll', 'with', 'a', 'yours', \"isn't\", 'from', 'wouldn', 'themselves', \"weren't\", 'ain', 'aren', 'has', 'those', 'it', 'our', 'was', 'herself', 'him', 'down', 'i', 'such', \"you're\", 't', 'couldn', 'mustn', 'shouldn', 'an', 'you', 'through', 'any', 'this', 'that', 'have', \"it's\", 'been', 'won', 'then', \"hadn't\", 'were', 'into', 'when', 'my', 'haven', 'itself', 'm', 'do', \"mightn't\", 'only', 'them', 'under', 'now', 'd', 'than', 'did', 'weren', 'hers', 'further', 'by', 'will', \"didn't\", \"wouldn't\", 'in', 'we', 'who'}\n"
     ]
    }
   ],
   "source": [
    "eng_words=set(stopwords.words(\"english\"))\n",
    "print(eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8a893cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "of\n",
      "the\n",
      "and\n",
      "most\n",
      "for\n",
      "and\n",
      "you\n",
      "can\n",
      "the\n",
      "of\n",
      "in\n",
      "the\n",
      "from\n",
      "a\n",
      "you\n",
      "can\n",
      "your\n",
      "into\n",
      "and\n",
      "then\n",
      "the\n",
      "if\n",
      "it\n",
      "in\n",
      "the\n",
      "of\n",
      "by\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(para)\n",
    "for i in words:\n",
    "    if i in eng_words:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd6898",
   "metadata": {},
   "source": [
    "5. From the above paragraph print frequency of each word using NLTK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f9f2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5), ('stop', 4), ('words', 4), ('NLTK', 3), ('of', 3), ('and', 3), ('.', 3), ('word', 2), (',', 2), ('you', 2), ('can', 2), ('list', 2), ('in', 2), ('remove', 2), ('The', 1), ('library', 1), ('is', 1), ('one', 1), ('oldest', 1), ('most', 1), ('commonly', 1), ('used', 1), ('Python', 1), ('libraries', 1), ('for', 1), ('Natural', 1), ('Language', 1), ('Processing', 1), ('supports', 1), ('removal', 1), ('find', 1), ('corpus', 1), ('module', 1), ('To', 1), ('from', 1), ('a', 1), ('sentence', 1), ('divide', 1), ('your', 1), ('text', 1), ('into', 1), ('then', 1), ('if', 1), ('it', 1), ('exits', 1), ('provided', 1), ('by', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fq=FreqDist(words)\n",
    "print(fq.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df18c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
